---
title: LLM
description: Use language models as tools with retry logic
---

In A1, LLMs are tools! They support structured output, retry logic with exponential backoff, and tool calling.

## Basic Usage

```python
from a1 import LLM, Agent

llm = LLM("gpt-4.1")

agent = Agent(
    name="writer",
    description="Writes content",
    tools=[llm, other_tool]
)
```

## Structured Output

```python
from pydantic import BaseModel

class Answer(BaseModel):
    result: float
    explanation: str

# Typed LLM tool
llm = LLM(
    model="groq:openai/gpt-oss-20b",
    input_schema=MathProblem,  # Optional: validates input
    output_schema=MathAnswer   # Optional: structured output
)
```

## Retry Strategy

By default, LLM tools use **RetryStrategy with 3 parallel candidates and 3 retries each**:

```python
from a1 import LLM, RetryStrategy

# Default: 3 candidates Ã— 3 retries = up to 9 attempts
llm = LLM("groq:openai/gpt-oss-20b")

# Custom retry configuration
llm = LLM(
    model="groq:openai/gpt-oss-20b",
    retry_strategy=RetryStrategy(
        max_iterations=5,      # Retries per candidate
        num_candidates=2       # Parallel candidates
    )
)
```

## Structured Output

LLM tools support JSON schema-based structured output:

```python
from pydantic import BaseModel

class ParsedData(BaseModel):
    entities: List[str]
    sentiment: str
    confidence: float

# LLM will return ParsedData instance
llm = LLM("groq:openai/gpt-oss-20b", output_schema=ParsedData)

# In generated code or runtime execution:
result = await llm.execute(
    content="Analyze: The product is amazing!",
    output_schema=ParsedData
)
# result is ParsedData(entities=[...], sentiment="positive", confidence=0.95)
```

## Function Calling

LLM tools support multi-provider function calling:

```python
from a1 import Agent, LLM, tool

@tool(name="search", description="Search the web")
async def search(query: str) -> str:
    return f"Results for: {query}"

# Agent with LLM + tools
agent = Agent(
    name="researcher",
    input_schema=Question,
    output_schema=Answer,
    tools=[
        search,
        LLM("groq:openai/gpt-oss-20b")
    ]
)

# Generated code can call tools via LLM:
# result = await llm(content="Search for Python tutorials", tools=[search])
```

## Provider Support

Any provider supported by Mozilla's `any-llm` SDK:

- **OpenAI**: `gpt-4.1`, `gpt-4.1`, `gpt-4.1-mini`
- **Anthropic**: `claude-sonnet-4`, `claude-haiku-4-5`
- **Google**: `gemini-2.0-flash`, `gemini-pro`
- **Groq**: `groq:llama-3.3-70b`, `groq:openai/gpt-oss-20b`

Use prefix with provider name if ambiguous:
```python
llm = LLM("groq:openai/gpt-oss-20b")  # Groq-hosted model
llm = LLM("anthropic:claude-sonnet-4")  # Anthropic
```

## Context Tracking

LLM calls automatically append to context:

- **User messages**: Input prompts
- **Assistant messages**: LLM responses
- **Tool messages**: Function call results

```python
from a1 import get_context

# After LLM execution
ctx = get_context("main")
print(f"Messages in context: {len(ctx)}")
for msg in ctx:
    print(f"{msg.role}: {msg.content[:50]}...")
```

See [Context & History](context.md) for details on when messages are appended.

## See Also

- [Agents](agents.md) - Using LLMs in agents
- [Retry & Validation](../advanced/retry.md) - Advanced retry configuration
- [Context & History](context.md) - Message tracking
